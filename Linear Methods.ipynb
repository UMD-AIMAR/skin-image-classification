{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>File Information</h2>\n",
    "\n",
    "10k images from the challenge website: https://challenge2018.isic-archive.com/task3/.<br>\n",
    "All the images are 600x450 and are somewhat centered on the skin lesion.<br>\n",
    "\n",
    "<ul>\n",
    "Folder structure:\n",
    "    <li>../Skin_Image_Recognition</li>\n",
    "    <ul>\n",
    "        <li>(this notebook).ipynb</li>\n",
    "        <li>target.csv</li>\n",
    "        <li>images</li>\n",
    "        <ul>\n",
    "            <li>img1.jpeg</li>\n",
    "            <li>img2.jpeg</li>\n",
    "            <li>...</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<h2>Define all functions at top, execute code at the bottom</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading Data</h2>\n",
    "\n",
    "`import_images(path)` loads all the images inside the specified folder and return them as a list of grayscale 2d matrices.\n",
    "\n",
    "`import_target(path)` loads a local csv that has classifications listed in the same order as the files in images. it returns list of 0s and 1s, indicating \"not nevus\" and \"nevus\".\n",
    "\n",
    "`drop_even(X, Y, N, M)` removes instances from the dataset to give a desired positive:negative classification ratio.\n",
    "\n",
    "To use `N` data points: `X_all, Y_all = drop_even(*import_data(), N)` <br>\n",
    "To use all data points: `X_all, Y_all = import_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "def import_images(path):\n",
    "    return np.array([cv2.imread(os.path.join(path, img_name), 0) for img_name in os.listdir(path)])\n",
    "\n",
    "def import_target(path):\n",
    "    f = open(os.path.join(path, \"target.csv\"))\n",
    "    lines = f.read().split('\\n')\n",
    "    toks = [line.split(',') for line in lines]\n",
    "    toks = toks[1:-1]\n",
    "    return np.array([int(float(tok[2])) for tok in toks])\n",
    "\n",
    "def import_data():\n",
    "    X_all = import_images(os.path.join(\"images\"))\n",
    "    Y_all = import_target(\"\")\n",
    "    Y_all = Y_all[:len(X_all)]\n",
    "    return X_all, Y_all\n",
    "\n",
    "def drop_even(X, Y, M, N):\n",
    "    indices = []\n",
    "    for i in range(len(X)):\n",
    "        if (Y[i] == 1 and M > 0):\n",
    "            M -= 1\n",
    "            indices.append(i)\n",
    "        if (Y[i] == 0 and N > 0):\n",
    "            N -= 1\n",
    "            indices.append(i)\n",
    "    return X[indices], Y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#X, Y = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 4\n",
    "#plt.imshow(X[n], cmap=(\"gray\"))\n",
    "#print(Y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing/Training Dataset Generation</h2>\n",
    "\n",
    "`linearize(X)` accepts X, a list of 2d matrices. The function flattens each 2d image matrix into a 1d vector that can be fed into a classifier.\n",
    "\n",
    "`shuffle(X,Y)` randomly permutes the order of images.\n",
    "\n",
    "`split(X,Y)` separates the training/testing data.\n",
    "\n",
    "`gen_train_test(X,Y,seed,r,verbose)` uses `shuffle` and `split` to generate a testing/training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize(X):\n",
    "    return np.array([x.flatten() for x in X])\n",
    "\n",
    "def shuffle(X,Y):\n",
    "    shuffle = np.random.permutation(len(X))\n",
    "    X_s, Y_s = X[shuffle], Y[shuffle]\n",
    "    return X_s, Y_s\n",
    "\n",
    "def split(X,Y,r):\n",
    "    c = int(len(X)*r) #cutoff for test/train data\n",
    "    return X[:c], Y[:c], X[c:], Y[c:]\n",
    "\n",
    "def gen_train_test(X,Y,seed,r,verbose):\n",
    "    np.random.seed(seed)\n",
    "    X_train, Y_train, X_test, Y_test = split(*shuffle(X,Y), r)\n",
    "    if verbose:\n",
    "        print(\"Train Non-Nevi:\", len(Y_train[Y_train == 0]), \"Train Nevi:\", len(Y_train[Y_train == 1]))\n",
    "        print(\"Test  Non-Nevi:\", len(Y_test[Y_test == 0]),   \"Test  Nevi:\", len(Y_test[Y_test == 1]))\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X.shape)\n",
    "#print(linearize(X).shape)\n",
    "\n",
    "# change the seed/split around\n",
    "#X_train, Y_train, X_test, Y_test = gen_train_test(X,Y,0,0.8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SGDClassifier</h2>\n",
    "\n",
    "`val_test(clf, X_test, Y_test)` takes a classifier and the testing data, and returns the accuracy from evaluating the testing datapoints.\n",
    "\n",
    "The commented sections inside `train_sgd_clf` print out results for 4-fold cross-validation and the resulting confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, confusion_matrix\n",
    "\n",
    "def val_test(clf, X_test, Y_test):\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    return sum(Y_pred == Y_test) / len(Y_pred)\n",
    "\n",
    "# misc code\n",
    "def train_sgd_clf(clf,X,Y,seed):\n",
    "    X_train, Y_train, X_test, Y_test = gen_train_test(X,Y,seed,0.8,verbose=True)\n",
    "    X_train = linearize(X_train)\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    cvs = cross_val_score(clf, X_train, Y_train, cv=4, scoring=\"accuracy\")\n",
    "    print(\"Cross validation scores:\", cvs)\n",
    "    \n",
    "    Y_train_pred = cross_val_predict(clf, X_train, Y_train, cv=4)\n",
    "    cm = confusion_matrix(Y_train, Y_train_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    acc = val_test(clf, linearize(X_test), Y_test)\n",
    "    print(\"Testing Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = SGDClassifier(loss='hinge', max_iter=4, tol=-np.infty, random_state=13)\n",
    "#train_sgd_clf(clf, X, Y, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_d, Y_d = drop_even(X, Y, 103, 103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show seed 1, seed 2\n",
    "#clf = SGDClassifier(loss='hinge', max_iter=4, tol=-np.infty, random_state=13)\n",
    "#train_sgd_clf(clf, X_d, Y_d, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:<br>\n",
    "[TN FP]<br>[FN TP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Downscaling</h2>\n",
    "\n",
    "All the images here are given as 600x450. Let's try resizing to 200x150 with different interpolation algorithms and seeing how performance changes.<br>\n",
    "Runtime is better, and accuracy stays similar for multiple interpolation methods (for the base images, at least)\n",
    "\n",
    "`resize_all` downscales `X`, the list of 2d matrices, based on `inter_type`, a `cv2.Interpolation_Flag`.\n",
    "\n",
    "`test_downscaling` takes in `funcs`, a list of `cv2.Interpolation_Flag` and trains models off of each downscaled dataset.<br>\n",
    "e.g. `test_downscaling(13, [cv2.INTER_LINEAR, cv2.INTER_NEAREST, cv2.INTER_CUBIC])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_all(X, inter_type):\n",
    "    return np.array([cv2.resize(x,dsize=(200,150), interpolation=inter_type) for x in X])\n",
    "\n",
    "def test_downscaling(seed, funcs):\n",
    "    for func in funcs:\n",
    "        np.random.seed(seed)\n",
    "        X_down = resize_all(X_all, func)\n",
    "        X_train, Y_train, X_test, Y_test = split(*shuffle(linearize(X_down), Y_all))\n",
    "        sgd_clf = SGDClassifier(loss='hinge', max_iter=4, tol=-np.infty, random_state=13) # hinge = SVM\n",
    "        train_sgd_clf(sgd_clf, X_train, Y_train)\n",
    "        print(val_test(sgd_clf, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Image Shifting</h2>\n",
    "\n",
    "We can artificially create more data locally by shifting images around (the same as cropping out x pixels on a particular side) and resizing, which gives us a new image that has the same classification as the original.\n",
    "\n",
    "`shift_img` crops out `n` pixels off a certain side given by `shift_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_img(img, n, shift_dir): # n = shift magnitude of pixels\n",
    "    m = img\n",
    "    if shift_dir == 0: # right\n",
    "        m = img[:-n, :]\n",
    "    if shift_dir == 1:\n",
    "        m = img[:-n, n:]\n",
    "    elif shift_dir == 2: # up\n",
    "        m = img[:, n:]\n",
    "    elif shift_dir == 3:\n",
    "        m = img[n:, n:]\n",
    "    elif shift_dir == 4: # left\n",
    "        m = img[n:, :]\n",
    "    elif shift_dir == 5:\n",
    "        m = img[n:, :-n]\n",
    "    elif shift_dir == 6: # down\n",
    "        m = img[:, :-n]\n",
    "    elif shift_dir == 7:\n",
    "        m = img[:-n, :-n]\n",
    "    return cv2.resize(m,dsize=(200,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = X_d[0]\n",
    "#print(img)\n",
    "#plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_img = shift_img(img, 12, 0)\n",
    "#print(new_img)\n",
    "#plt.imshow(new_img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch Generation</h2>\n",
    "\n",
    "When too many images are generated (using the shifting transformation), the computer does not have enough RAM to store them. To prevent this, we generate one 'batch' of images at a time and apply `partial_fit` to our classifier repeatedly.\n",
    "\n",
    "`gen_batches` creates a permutation of all integers from 1 to (total images) and splits it into separate lists of equal size (batches).\n",
    "\n",
    "`gen_img` creates the image mapped to the integer value from the batch.\n",
    "\n",
    "`gen_XY_batch` does `gen_img` for all images and updates Y (the observations) accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches(n, batch_size, m): # m = multiplier, number of shift magnitudes * 8 + 1\n",
    "    ids = np.random.permutation(n*m)\n",
    "    num_batches = n*m//batch_size\n",
    "    full_batches = [ids[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]\n",
    "    last_batch = ids[batch_size*num_batches:]\n",
    "    if len(last_batch) > 0:\n",
    "        full_batches.append(last_batch)\n",
    "    return full_batches\n",
    "\n",
    "def gen_img(X, img_id, shifts):\n",
    "    s = len(shifts)*8 + 1\n",
    "    img = X[img_id // s]\n",
    "    shift_type = img_id % s\n",
    "    if shift_type == s-1:\n",
    "        return img\n",
    "    shift_mag, shift_dir = shifts[shift_type // 8], shift_type % 8\n",
    "    return shift_img(img, shift_mag, shift_dir)\n",
    "\n",
    "def gen_XY_batch(X,Y,batch,shifts):\n",
    "    return np.array([gen_img(X, img_id, shifts) for img_id in batch]), Y[batch // (len(shifts)*8+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_batches(20, 5, 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Partial Fitting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train_aug(clf, X_train, Y_train, seed, epochs, batch_size, shifts):\n",
    "    np.random.seed(seed)\n",
    "    classes = np.unique(Y_train)\n",
    "    for n in range(epochs):\n",
    "        batches = gen_batches(len(X_train), batch_size, len(shifts)*8 + 1)\n",
    "        for batch in batches:\n",
    "            X_train_b, Y_train_b = gen_XY_batch(X_train, Y_train, batch, shifts)\n",
    "            clf.partial_fit(linearize(X_train_b), Y_train_b, classes)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = SGDClassifier(loss='hinge', max_iter=4, tol=-np.infty, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running Trials</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "def grid_search_aug(clf, X, Y, seed_range, epoch_range, batch_size_range, shift_sets):\n",
    "    for seed in seed_range:\n",
    "        X_train, Y_train, X_test, Y_test = gen_train_test(X, Y, seed, 0.8, verbose=True)\n",
    "        X_train_l = linearize(X_train)\n",
    "        for shift_set in shift_sets:\n",
    "            for epochs in epoch_range:\n",
    "                for batch_size in batch_size_range:\n",
    "                    clf_clone = clone(clf)\n",
    "                    batch_train_aug(clf_clone, X_train, Y_train, seed, epochs, batch_size, shift_set)\n",
    "                    \n",
    "                    acc = val_test(clf_clone, linearize(X_test), Y_test)\n",
    "                    cvs = 0\n",
    "                    #cvs = cross_val_score(clf_clone, X_train_l, Y_train, cv=4, scoring=\"accuracy\")\n",
    "                    \n",
    "                    s = \"{:<.5f}\\t{:<6d}\\t{:<6d}\\t{:<6d}\\t{}\\t{}\"\n",
    "                    print(s.format(acc, seed, epochs, batch_size, shift_set, cvs))\n",
    "\n",
    "                    \n",
    "#X_train is linearized                    \n",
    "def grid_search(clf, X, Y, seed_range):\n",
    "    for seed in seed_range:\n",
    "        X_train, Y_train, X_test, Y_test = gen_train_test(X, Y, seed, 0.8, verbose=False)\n",
    "        X_train_l = linearize(X_train)\n",
    "        \n",
    "        clf_clone = clone(clf)\n",
    "        clf_clone.fit(X_train_l, Y_train)\n",
    "\n",
    "        acc = val_test(clf_clone, linearize(X_test), Y_test)\n",
    "#        cvs = 0\n",
    "        cvs = cross_val_score(clf_clone, X_train_l, Y_train, cv=4, scoring=\"accuracy\")\n",
    "\n",
    "        s = \"{:<.5f}\\t{:<6d}\\t{}\"\n",
    "        print(s.format(acc, seed, cvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Misc</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, n_comp):\n",
    "    pca = PCA(n_components = n_comp)\n",
    "    pca.fit(X)\n",
    "    return pca.fit_transform(X)\n",
    "\n",
    "def test_pca():\n",
    "    X_pca = pca(linearize(X_all), n_comp)\n",
    "    for n_comp in range(5,100,5):\n",
    "        np.random.seed(13)\n",
    "        #X_train, Y_train, X_test, Y_test = split(*shuffle(linearize(X_all), Y_all))\n",
    "        X_train, Y_train, X_test, Y_test = split(*shuffle(X_pca, Y_all))\n",
    "        print(\"Training, total:\", len(Y_train), \"negative:\", len(Y_train[Y_train == 0]))\n",
    "        print(\"Testing, total:\", len(Y_test), \"negative:\", len(Y_test[Y_test == 0]))\n",
    "\n",
    "        sgd_clf = SGDClassifier(loss='hinge', max_iter=4, tol=-np.infty, random_state=13) # hinge = SVM\n",
    "        train_sgd_clf(sgd_clf, X_train, Y_train)\n",
    "        print(\"600x450 Accuracy:\", val_test(sgd_clf, X_test, Y_test), n_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Code goes here</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Lines: 206 Non-Nevi: 103 Nevi: 103\n",
      "Images: 206\n"
     ]
    }
   ],
   "source": [
    "X_all, Y_all = drop_even(*import_data(), 103, 103)\n",
    "X_all = resize_all(X_all, cv2.INTER_LINEAR)\n",
    "print(\"CSV Lines:\", len(Y_all), \"Non-Nevi:\", len(Y_all[Y_all == 0]), \"Nevi:\", len(Y_all[Y_all == 1]))\n",
    "print(\"Images:\", len(X_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Non-Nevi: 82 Train Nevi: 82\n",
      "Test  Non-Nevi: 21 Test  Nevi: 21\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = gen_train_test(linearize(X_all),Y_all,17,0.8,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kevin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\kevin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'loss': 'hinge', 'penalty': 'l2'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.744 (+/-0.163) for {'loss': 'hinge', 'penalty': 'none'}\n",
      "0.749 (+/-0.135) for {'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.744 (+/-0.163) for {'loss': 'hinge', 'penalty': 'l1'}\n",
      "0.702 (+/-0.307) for {'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "0.724 (+/-0.132) for {'loss': 'log', 'penalty': 'none'}\n",
      "0.650 (+/-0.414) for {'loss': 'log', 'penalty': 'l2'}\n",
      "0.724 (+/-0.132) for {'loss': 'log', 'penalty': 'l1'}\n",
      "0.741 (+/-0.137) for {'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.19      0.32        21\n",
      "           1       0.55      1.00      0.71        21\n",
      "\n",
      "   micro avg       0.60      0.60      0.60        42\n",
      "   macro avg       0.78      0.60      0.52        42\n",
      "weighted avg       0.78      0.60      0.52        42\n",
      "\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'loss': 'log', 'penalty': 'none'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.683 (+/-0.160) for {'loss': 'hinge', 'penalty': 'none'}\n",
      "0.671 (+/-0.195) for {'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.683 (+/-0.160) for {'loss': 'hinge', 'penalty': 'l1'}\n",
      "0.659 (+/-0.223) for {'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "0.707 (+/-0.145) for {'loss': 'log', 'penalty': 'none'}\n",
      "0.616 (+/-0.191) for {'loss': 'log', 'penalty': 'l2'}\n",
      "0.707 (+/-0.145) for {'loss': 'log', 'penalty': 'l1'}\n",
      "0.665 (+/-0.137) for {'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.50      1.00      0.67        21\n",
      "\n",
      "   micro avg       0.50      0.50      0.50        42\n",
      "   macro avg       0.25      0.50      0.33        42\n",
      "weighted avg       0.25      0.50      0.33        42\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kevin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "c:\\users\\kevin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tuned_parameters = [{'loss': ['hinge', 'log'], \n",
    "                     'penalty': ['none', 'l2', 'l1', 'elasticnet']}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    clf = GridSearchCV(SGDClassifier(max_iter=5, tol=-np.infty, random_state=0), tuned_parameters, cv=5,\n",
    "                           scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    Y_true, Y_pred = Y_test, clf.predict(X_test)\n",
    "    print(classification_report(Y_true, Y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD on regular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc    \tSeed  \tCrossval\n",
      "0.54762\t0     \t[0.64285714 0.47619048 0.625      0.6       ]\n",
      "0.47619\t1     \t[0.5        0.73170732 0.65853659 0.525     ]\n",
      "0.73810\t2     \t[0.57142857 0.63414634 0.51219512 0.5       ]\n",
      "0.57143\t3     \t[0.57142857 0.6097561  0.51219512 0.475     ]\n",
      "0.57143\t4     \t[0.7804878  0.6097561  0.63414634 0.56097561]\n",
      "0.57143\t5     \t[0.64285714 0.68292683 0.53658537 0.65      ]\n",
      "0.52381\t6     \t[0.5        0.48780488 0.48780488 0.575     ]\n",
      "0.57143\t7     \t[0.73809524 0.51219512 0.58536585 0.6       ]\n",
      "0.52381\t8     \t[0.57142857 0.53658537 0.6097561  0.7       ]\n",
      "0.73810\t9     \t[0.47619048 0.6097561  0.6097561  0.6       ]\n",
      "0.73810\t10    \t[0.48780488 0.53658537 0.48780488 0.48780488]\n",
      "0.52381\t11    \t[0.52380952 0.58536585 0.63414634 0.5       ]\n",
      "0.69048\t12    \t[0.80952381 0.6097561  0.70731707 0.525     ]\n",
      "0.54762\t13    \t[0.7804878  0.51219512 0.68292683 0.48780488]\n",
      "0.52381\t14    \t[0.5952381  0.6097561  0.51219512 0.575     ]\n",
      "0.69048\t15    \t[0.69047619 0.51219512 0.51219512 0.625     ]\n",
      "0.61905\t16    \t[0.52380952 0.6097561  0.68292683 0.725     ]\n",
      "0.61905\t17    \t[0.57142857 0.5952381  0.775      0.55      ]\n",
      "0.40476\t18    \t[0.52380952 0.66666667 0.725      0.475     ]\n",
      "0.42857\t19    \t[0.54761905 0.51219512 0.51219512 0.825     ]\n"
     ]
    }
   ],
   "source": [
    "strs = [\"Acc\", \"Seed\", \"Crossval\"]\n",
    "print(\"{:<7}\\t{:<6}\\t{}\".format(*strs))\n",
    "grid_search(\n",
    "    clf = SGDClassifier(loss=\"hinge\", max_iter=4, tol=-np.infty, random_state=13), \n",
    "    X = X_all,\n",
    "    Y = Y_all,\n",
    "    seed_range = [x for x in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Non-Nevi: 86 Train Nevi: 78\n",
      "Test  Non-Nevi: 17 Test  Nevi: 25\n"
     ]
    }
   ],
   "source": [
    "# Showing how to retrieve a test case\n",
    "#X_train, Y_train, X_test, Y_test = gen_train_test(X_all, Y_all, 18, 0.8, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Non-Nevi: 86 Train Nevi: 78\n",
      "Test  Non-Nevi: 17 Test  Nevi: 25\n",
      "Cross validation scores: [0.52380952 0.66666667 0.725      0.475     ]\n",
      "Confusion Matrix:\n",
      "[[45 41]\n",
      " [25 53]]\n",
      "Testing Accuracy: 0.40476190476190477\n"
     ]
    }
   ],
   "source": [
    "#clf = SGDClassifier(loss=\"hinge\", max_iter=4, tol=-np.infty, random_state=13)\n",
    "#train_sgd_clf(clf,X_all,Y_all,18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP on regular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "strs = [\"Acc\", \"Seed\", \"Crossval\"]\n",
    "print(\"{:<7}\\t{:<6}\\t{}\".format(*strs))\n",
    "grid_search(\n",
    "    clf = MLPClassifier(random_state=13), \n",
    "    X = X_all,\n",
    "    Y = Y_all,\n",
    "    seed_range = [x for x in range(0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD on shifted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs = [\"Acc\", \"Seed\", \"Epochs\", \"Batch\"]\n",
    "print(\"{:<7}\\t{:<6}\\t{:<6}\\t{:<6}\".format(*strs))\n",
    "shift_sets = [[3], [3,6], [3,6,9], [3,6,9,12], [3,6,9,12,15]]\n",
    "grid_search_aug(\n",
    "    clf = SGDClassifier(loss=\"hinge\", max_iter=4, tol=-np.infty, random_state=13), \n",
    "    X = X_all,\n",
    "    Y = Y_all,\n",
    "    seed_range = [x for x in range(0)], \n",
    "    epoch_range = [1,2,3,4], \n",
    "    batch_size_range = [50, 100], \n",
    "    shift_sets = shift_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
