{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "def import_images(path, img_names):\n",
    "    imgs = [cv2.imread(os.path.join(path, img_name), 1) for img_name in img_names]\n",
    "    return np.array(resize_all(imgs))\n",
    "\n",
    "def import_target(path):\n",
    "    f = open(path)\n",
    "    lines = f.read().split('\\n')\n",
    "    toks = [line.split(',') for line in lines]\n",
    "    toks = toks[1:-1]\n",
    "    f.close()\n",
    "    return np.array([[int(tk) for tk in tok[1:]] for tok in toks])\n",
    "\n",
    "def resize_all(X):\n",
    "    return np.array([cv2.resize(x,dsize=(200,150)) for x in X])\n",
    "\n",
    "def data_generator(path, total, img_names, targets, batch_size):\n",
    "    i = 0\n",
    "    N = np.random.permutation(total)\n",
    "    while True:\n",
    "        batch = N[i:i+batch_size]\n",
    "        X = load_images(path, img_names[batch])\n",
    "        Y = targets[batch]\n",
    "        yield X, Y\n",
    "        i = (i+batch_size) % total\n",
    "\n",
    "def class_indices(Y, j):\n",
    "    return [i for i in range(len(Y)) if Y[i][j] == 1]\n",
    "\n",
    "def loading_text(text):\n",
    "    sys.stdout.write(str(text) + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"data\\\\ISIC2018_Task3_Training_Input\"\n",
    "target_path = \"data\\\\ISIC_2018_Training_GroundTruth.csv\"\n",
    "img_names, Y_all = np.array(os.listdir(img_path)), import_target(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.99820305,  1.49366145, 19.4844358 , 30.62691131,  9.11282985,\n",
       "       87.08695652, 70.52816901])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = np.sum(Y_all, axis=0)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe augment a bit, but need to see if unbalanced works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = np.zeros(shape=(7000, 150, 200, 3)), np.zeros(shape=(7000, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(0, 7):\n",
    "#     inds = class_indices(Y_all, j)[:1000]\n",
    "#     X_c = import_images(path, img_names[inds])\n",
    "#     Y_c = Y_all[inds]\n",
    "#     i = len(inds)\n",
    "#     X[1000*j:1000*j+i] = X_c\n",
    "#     Y[1000*j:1000*j+i] = Y_c   \n",
    "#     if i < 1000:  \n",
    "#         for x_new, y_new in datagen.flow(X_c, Y_c, batch_size=1):\n",
    "#             X[1000*j+i] = x_new\n",
    "#             Y[1000*j+i] = y_new\n",
    "#             i += 1\n",
    "#             if i == 1000:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(Y, axis=0))\n",
    "# print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\kevin\\documents\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "include_top=False,\n",
    "input_shape=(150, 200, 3))\n",
    "#conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(img_names, batch_size, conv_base):\n",
    "    total = len(img_names)\n",
    "    output_shape = conv_base.layers[-1].output_shape[1:]\n",
    "    features = np.zeros(shape=(total,) + output_shape)\n",
    "    i = 0\n",
    "    while i*batch_size < total:\n",
    "        loading_text(str(i*batch_size) +  \"/\" + str(total))\n",
    "        img_names_batch = img_names[i*batch_size:(i+1)*batch_size]\n",
    "        inputs_batch = import_images(img_path, img_names_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = conv_base.predict(inputs_batch)\n",
    "        i += 1\n",
    "    features = features.reshape(total, np.prod(output_shape))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f = load_features(img_names, 350, conv_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# file = open('10015_VGG.pkl', 'wb')\n",
    "# pickle.dump(X_f, file)\n",
    "file = open('10015_VGG.pkl', 'rb')\n",
    "X_f = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_f, Y_all,\n",
    "                                                stratify=Y_all, \n",
    "                                                test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8012, 12288), (2003, 12288), (8012, 7), (2003, 7))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 890, 5364,  411,  262,  879,   92,  114]),\n",
       " array([ 223, 1341,  103,   65,  220,   23,   28]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Y_train, axis=0), np.sum(Y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8012 samples, validate on 2003 samples\n",
      "Epoch 1/40\n",
      "8012/8012 [==============================] - 51s 6ms/step - loss: 0.9972 - acc: 0.6787 - val_loss: 0.8829 - val_acc: 0.6945\n",
      "Epoch 2/40\n",
      "8012/8012 [==============================] - 44s 5ms/step - loss: 0.8156 - acc: 0.7197 - val_loss: 0.7879 - val_acc: 0.7314\n",
      "Epoch 3/40\n",
      "8012/8012 [==============================] - 43s 5ms/step - loss: 0.7579 - acc: 0.7435 - val_loss: 0.7647 - val_acc: 0.7369\n",
      "Epoch 4/40\n",
      "8012/8012 [==============================] - 44s 5ms/step - loss: 0.7135 - acc: 0.7592 - val_loss: 0.7478 - val_acc: 0.7469\n",
      "Epoch 5/40\n",
      "8012/8012 [==============================] - 45s 6ms/step - loss: 0.6814 - acc: 0.7747 - val_loss: 0.7317 - val_acc: 0.7519\n",
      "Epoch 6/40\n",
      "8012/8012 [==============================] - 47s 6ms/step - loss: 0.6616 - acc: 0.7837 - val_loss: 0.7270 - val_acc: 0.7564\n",
      "Epoch 7/40\n",
      "8012/8012 [==============================] - 50s 6ms/step - loss: 0.6238 - acc: 0.7973 - val_loss: 0.7184 - val_acc: 0.7594\n",
      "Epoch 8/40\n",
      "8012/8012 [==============================] - 54s 7ms/step - loss: 0.6060 - acc: 0.8025 - val_loss: 0.7627 - val_acc: 0.7579\n",
      "Epoch 9/40\n",
      "8012/8012 [==============================] - 61s 8ms/step - loss: 0.5828 - acc: 0.8155 - val_loss: 0.7493 - val_acc: 0.7614\n",
      "Epoch 10/40\n",
      "8012/8012 [==============================] - 67s 8ms/step - loss: 0.5475 - acc: 0.8280 - val_loss: 0.7327 - val_acc: 0.7718\n",
      "Epoch 11/40\n",
      "8012/8012 [==============================] - 64s 8ms/step - loss: 0.5326 - acc: 0.8321 - val_loss: 0.7489 - val_acc: 0.7649\n",
      "Epoch 12/40\n",
      "8012/8012 [==============================] - 60s 7ms/step - loss: 0.5005 - acc: 0.8414 - val_loss: 0.7372 - val_acc: 0.7639\n",
      "Epoch 13/40\n",
      "8012/8012 [==============================] - 55s 7ms/step - loss: 0.4868 - acc: 0.8494 - val_loss: 0.7273 - val_acc: 0.7738\n",
      "Epoch 14/40\n",
      "8012/8012 [==============================] - 52s 7ms/step - loss: 0.4555 - acc: 0.8611 - val_loss: 0.7283 - val_acc: 0.7748\n",
      "Epoch 15/40\n",
      "8012/8012 [==============================] - 52s 7ms/step - loss: 0.4425 - acc: 0.8612 - val_loss: 0.7543 - val_acc: 0.7738\n",
      "Epoch 16/40\n",
      "8012/8012 [==============================] - 52s 6ms/step - loss: 0.4173 - acc: 0.8719 - val_loss: 0.7520 - val_acc: 0.7818\n",
      "Epoch 17/40\n",
      "8012/8012 [==============================] - 53s 7ms/step - loss: 0.4056 - acc: 0.8759 - val_loss: 0.7968 - val_acc: 0.7693\n",
      "Epoch 18/40\n",
      "8012/8012 [==============================] - 55s 7ms/step - loss: 0.3954 - acc: 0.8826 - val_loss: 0.7914 - val_acc: 0.7723\n",
      "Epoch 19/40\n",
      "8012/8012 [==============================] - 56s 7ms/step - loss: 0.3693 - acc: 0.8888 - val_loss: 0.8051 - val_acc: 0.7703\n",
      "Epoch 20/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.3662 - acc: 0.8943 - val_loss: 0.8213 - val_acc: 0.7723\n",
      "Epoch 21/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.3375 - acc: 0.8995 - val_loss: 0.8558 - val_acc: 0.7733\n",
      "Epoch 22/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.3244 - acc: 0.9033 - val_loss: 0.8393 - val_acc: 0.7803\n",
      "Epoch 23/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.3113 - acc: 0.9090 - val_loss: 0.8391 - val_acc: 0.7753\n",
      "Epoch 24/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.3007 - acc: 0.9119 - val_loss: 0.8765 - val_acc: 0.7783\n",
      "Epoch 25/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.2968 - acc: 0.9153 - val_loss: 0.9136 - val_acc: 0.7733\n",
      "Epoch 26/40\n",
      "8012/8012 [==============================] - 63s 8ms/step - loss: 0.2891 - acc: 0.9155 - val_loss: 0.9370 - val_acc: 0.7778\n",
      "Epoch 27/40\n",
      "8012/8012 [==============================] - 65s 8ms/step - loss: 0.2682 - acc: 0.9216 - val_loss: 0.8825 - val_acc: 0.7843\n",
      "Epoch 28/40\n",
      "8012/8012 [==============================] - 58s 7ms/step - loss: 0.2541 - acc: 0.9290 - val_loss: 0.9055 - val_acc: 0.7853\n",
      "Epoch 29/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.2574 - acc: 0.9272 - val_loss: 0.9078 - val_acc: 0.7823\n",
      "Epoch 30/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.2488 - acc: 0.9314 - val_loss: 0.9431 - val_acc: 0.7853\n",
      "Epoch 31/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.2342 - acc: 0.9357 - val_loss: 1.0203 - val_acc: 0.7788\n",
      "Epoch 32/40\n",
      "8012/8012 [==============================] - 58s 7ms/step - loss: 0.2083 - acc: 0.9451 - val_loss: 0.9555 - val_acc: 0.7863\n",
      "Epoch 33/40\n",
      "8012/8012 [==============================] - 58s 7ms/step - loss: 0.2070 - acc: 0.9446 - val_loss: 0.9547 - val_acc: 0.7868\n",
      "Epoch 34/40\n",
      "8012/8012 [==============================] - 58s 7ms/step - loss: 0.1933 - acc: 0.9468 - val_loss: 0.9944 - val_acc: 0.7908\n",
      "Epoch 35/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.2063 - acc: 0.9438 - val_loss: 1.0050 - val_acc: 0.7903\n",
      "Epoch 36/40\n",
      "8012/8012 [==============================] - 57s 7ms/step - loss: 0.1933 - acc: 0.9472 - val_loss: 1.0233 - val_acc: 0.7883\n",
      "Epoch 37/40\n",
      "8012/8012 [==============================] - 59s 7ms/step - loss: 0.1835 - acc: 0.9521 - val_loss: 1.0907 - val_acc: 0.7873\n",
      "Epoch 38/40\n",
      "8012/8012 [==============================] - 60s 7ms/step - loss: 0.1827 - acc: 0.9496 - val_loss: 0.9984 - val_acc: 0.7938\n",
      "Epoch 39/40\n",
      "8012/8012 [==============================] - 59s 7ms/step - loss: 0.1715 - acc: 0.9541 - val_loss: 1.0905 - val_acc: 0.7908\n",
      "Epoch 40/40\n",
      "8012/8012 [==============================] - 60s 7ms/step - loss: 0.1585 - acc: 0.9589 - val_loss: 1.0327 - val_acc: 0.7913\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "weights = {i:len(Y_all)/(counts[i]) for i in range(len(counts))}\n",
    "histories = []\n",
    "ms = []\n",
    "\n",
    "for i in range(1):\n",
    "    np.random.seed(2)\n",
    "    set_random_seed(2)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(512, activation='sigmoid', input_dim=X_train.shape[1]))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(7, activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy']) # adam optimizer?\n",
    "\n",
    "    history = model.fit(X_train, Y_train, epochs=40, batch_size=20, validation_data=(X_test,Y_test))\n",
    "    # history = model.fit(X_f, Y_all, epochs=40, batch_size=20, class_weight=weights)\n",
    "    histories.append(history)\n",
    "    ms.append(model)\n",
    "# history = model.fit(X_train, Y_train, epochs=30, batch_size=64, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "# plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend()\n",
    "# plt.figure()\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.49      0.54       223\n",
      "           1       0.86      0.94      0.90      1341\n",
      "           2       0.66      0.60      0.63       103\n",
      "           3       0.45      0.49      0.47        65\n",
      "           4       0.60      0.45      0.51       220\n",
      "           5       0.36      0.17      0.24        23\n",
      "           6       0.86      0.68      0.76        28\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      2003\n",
      "   macro avg       0.63      0.55      0.58      2003\n",
      "weighted avg       0.78      0.79      0.78      2003\n",
      " samples avg       0.79      0.79      0.79      2003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "for i in range(len(ms)):\n",
    "    print(\"Model #\" + str(i))\n",
    "    model = ms[i]\n",
    "    a = model.predict(X_test)\n",
    "    b = np.zeros_like(a) # lol so this was it all along\n",
    "    b[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    #Y_test.shape, Y_pred.shape\n",
    "    print(classification_report(Y_test, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms[0].save(\"models\\\\vgg16_sigmoid_10015.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
